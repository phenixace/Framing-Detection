{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from dataset import ClimateDataset\r\n",
    "\r\n",
    "dataset = \"./unlabelled_articles_17K/opinion_climate_all_with_bias.csv\"\r\n",
    "annotated_id_file = \"./annotated_data_500/pretty_0611_lcad.txt\"\r\n",
    "annotated_file = \"./annotated_data_500/0611_majority.json\"\r\n",
    "\r\n",
    "entire_dataset = ClimateDataset(dataset, annotated_id_file, annotated_file)\r\n",
    "labelled_dataset = entire_dataset.get_labelled_dataset()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from sentence_transformers import SentenceTransformer, util\r\n",
    "\r\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "queries = ['attribution of responsibility', 'human interest', 'morality', 'conflict', 'economy']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "query_embeddings = model.encode(queries)\r\n",
    "\r\n",
    "print(query_embeddings)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 0.01343607  0.03816285  0.02155725 ...  0.03995542  0.0181604\n",
      "  -0.02009618]\n",
      " [ 0.01507804  0.08343946 -0.00394326 ...  0.05272075  0.00399472\n",
      "   0.01211869]\n",
      " [ 0.02066687  0.06011951  0.0280279  ...  0.0401177   0.03867239\n",
      "   0.0159173 ]\n",
      " [ 0.05329098  0.01754071 -0.00333661 ...  0.03607184  0.02508171\n",
      "   0.03295524]\n",
      " [-0.04011215  0.11893874  0.01274614 ... -0.01535319 -0.00672154\n",
      "   0.00540853]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from nltk.tokenize import sent_tokenize\r\n",
    "\r\n",
    "sentences = sent_tokenize(labelled_dataset[2][0])\r\n",
    "embeddings = model.encode(sentences)\r\n",
    "print(labelled_dataset[2][1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "cos_sim = util.cos_sim(query_embeddings, embeddings)\r\n",
    "\r\n",
    "all_sentence_combinations = []\r\n",
    "for i in range(len(queries)):\r\n",
    "    for j in range(len(sentences)):\r\n",
    "        all_sentence_combinations.append([cos_sim[i][j], i, j])\r\n",
    "\r\n",
    "all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\r\n",
    "\r\n",
    "print(\"Top-5 most similar pairs:\")\r\n",
    "for score, i, j in all_sentence_combinations[0:5]:\r\n",
    "    print(\"{} \\t {} \\t {:.4f}\".format(queries[i], sentences[j], score))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Top-5 most similar pairs:\n",
      "economy \t The situation is chaotic people are running on the streets and buildings collapsed. \t 0.2419\n",
      "conflict \t The situation is chaotic people are running on the streets and buildings collapsed. \t 0.2398\n",
      "conflict \t Video on social media shows people screaming and fleeing in panic and a mosque amongst the buildings damaged. \t 0.1565\n",
      "economy \t Last month a series of earthquakes struck the Indonesian island of Lombok killing hundreds of people the biggest on 5 August killed more than 460. \t 0.1465\n",
      "attribution of responsibility \t There is a ship washed ashore she added. \t 0.1386\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Zero Shot Performance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "from torch.utils.data import Subset\r\n",
    "fold = 1\r\n",
    "test_set  = Subset(labelled_dataset, [i for i in range((3 + fold)%5, len(labelled_dataset), 5)])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def calc_em(input_dict):\r\n",
    "    preds = input_dict['y_pred']\r\n",
    "    truths = input_dict['y_true']\r\n",
    "    correct_1 = 0\r\n",
    "    total_1 = 0\r\n",
    "    correct_2 = 0\r\n",
    "    total_2 = 0\r\n",
    "    correct_a = 0\r\n",
    "    total_a = 0\r\n",
    "\r\n",
    "    for i in range(len(truths)):\r\n",
    "        if sum(truths[i]) == 1:\r\n",
    "            total_1 += 1\r\n",
    "            if preds[i] == truths[i]:\r\n",
    "                correct_1 += 1\r\n",
    "\r\n",
    "        elif sum(truths[i]) == 2:\r\n",
    "            total_2 += 1\r\n",
    "            if preds[i] == truths[i]:\r\n",
    "                correct_2 += 1\r\n",
    "\r\n",
    "        total_a += 1\r\n",
    "        if preds[i] == truths[i]:\r\n",
    "            correct_a += 1\r\n",
    "\r\n",
    "    return {'em-1': correct_1/total_1, 'em-2': correct_2/total_2, 'em-a': correct_a/total_a}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def find_frame_with_best_n(text, n):\r\n",
    "    sentences = sent_tokenize(text)\r\n",
    "    embeddings = model.encode(sentences)\r\n",
    "\r\n",
    "    cos_sim = util.cos_sim(query_embeddings, embeddings)\r\n",
    "\r\n",
    "    all_sentence_combinations = []\r\n",
    "    for i in range(len(queries)):\r\n",
    "        for j in range(len(sentences)):\r\n",
    "            all_sentence_combinations.append([cos_sim[i][j], i, j])\r\n",
    "\r\n",
    "    #Sort list by the highest cosine similarity score\r\n",
    "    all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\r\n",
    "\r\n",
    "    output = [0, 0, 0, 0, 0]\r\n",
    "    for score, i, j in all_sentence_combinations[:n]:\r\n",
    "        output[i] = 1\r\n",
    "\r\n",
    "    return output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "n = 2\r\n",
    "preds = []\r\n",
    "truths = []\r\n",
    "for i in range(0, len(test_set)):\r\n",
    "    preds.append(find_frame_with_best_n(test_set[i][0], n))\r\n",
    "    truths.append(test_set[i][1])\r\n",
    "\r\n",
    "input_dict = {\"y_true\": truths, \"y_pred\": preds}\r\n",
    "\r\n",
    "from utils import Evaluator\r\n",
    "\r\n",
    "evaluator = Evaluator(classifier='naive', mode='macro', detail=True)   \r\n",
    "print(calc_em(input_dict))\r\n",
    "evaluator.eval(input_dict)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'em-1': 0.1, 'em-2': 0.07407407407407407, 'em-a': 0.04}\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Precision': [0.34782608695652173,\n",
       "  0.20588235294117646,\n",
       "  0.09090909090909091,\n",
       "  0.6730769230769231,\n",
       "  0.4107142857142857],\n",
       " 'Recall': [0.18604651162790697,\n",
       "  0.25,\n",
       "  0.09090909090909091,\n",
       "  0.546875,\n",
       "  0.6571428571428571],\n",
       " 'F1': [0.24242424242424243,\n",
       "  0.22580645161290322,\n",
       "  0.09090909090909091,\n",
       "  0.603448275862069,\n",
       "  0.5054945054945055],\n",
       " 'Acc': [0.5, 0.52, 0.8, 0.54, 0.55]}"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "def find_frame(text, threshold):\r\n",
    "    sentences = sent_tokenize(text)\r\n",
    "    embeddings = model.encode(sentences)\r\n",
    "\r\n",
    "    cos_sim = util.cos_sim(query_embeddings, embeddings)\r\n",
    "\r\n",
    "    all_sentence_combinations = []\r\n",
    "    for i in range(len(queries)):\r\n",
    "        for j in range(len(sentences)):\r\n",
    "            all_sentence_combinations.append([cos_sim[i][j], i, j])\r\n",
    "\r\n",
    "    #Sort list by the highest cosine similarity score\r\n",
    "    all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\r\n",
    "\r\n",
    "    output = [0, 0, 0, 0, 0]\r\n",
    "    for score, i, j in all_sentence_combinations:\r\n",
    "        if score < threshold[i]:\r\n",
    "            break\r\n",
    "        else:\r\n",
    "            output[queries.index(queries[i])] = 1\r\n",
    "\r\n",
    "    return output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "threshold = [0.17] * 5\r\n",
    "\r\n",
    "preds = []\r\n",
    "truths = []\r\n",
    "for i in range(0, len(test_set)):\r\n",
    "    preds.append(find_frame(test_set[i][0], threshold))\r\n",
    "    truths.append(test_set[i][1])\r\n",
    "\r\n",
    "input_dict = {\"y_true\": truths, \"y_pred\": preds}\r\n",
    "print(calc_em(input_dict))\r\n",
    "from utils import Evaluator\r\n",
    "\r\n",
    "evaluator = Evaluator(classifier='naive', mode='macro', detail=True)   \r\n",
    "\r\n",
    "evaluator.eval(input_dict)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'em-1': 0.0, 'em-2': 0.030303030303030304, 'em-a': 0.0297029702970297}\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Precision': [0.4810126582278481,\n",
       "  0.2619047619047619,\n",
       "  0.0847457627118644,\n",
       "  0.7444444444444445,\n",
       "  0.3723404255319149],\n",
       " 'Recall': [0.76, 0.7857142857142857, 1.0, 0.9305555555555556, 1.0],\n",
       " 'F1': [0.5891472868217054,\n",
       "  0.39285714285714285,\n",
       "  0.15625,\n",
       "  0.8271604938271605,\n",
       "  0.5426356589147286],\n",
       " 'Acc': [0.4752475247524752,\n",
       "  0.32673267326732675,\n",
       "  0.46534653465346537,\n",
       "  0.7227722772277227,\n",
       "  0.4158415841584158]}"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Weakly supervised"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "from torch.utils.data import Subset\r\n",
    "from utils import Evaluator\r\n",
    "\r\n",
    "fold = 5\r\n",
    "\r\n",
    "train_set = Subset(labelled_dataset, [i for i in range((-1 + fold)%5, len(labelled_dataset), 5)] + [i for i in range((0 + fold)%5, len(labelled_dataset), 5)] + [i for i in range((1 + fold)%5, len(labelled_dataset), 5)])\r\n",
    "valid_set = Subset(labelled_dataset, [i for i in range((2 + fold)%5, len(labelled_dataset), 5)])\r\n",
    "test_set  = Subset(labelled_dataset, [i for i in range((3 + fold)%5, len(labelled_dataset), 5)])\r\n",
    "\r\n",
    "evaluator = Evaluator(classifier='naive', mode='macro', detail=True)\r\n",
    "\r\n",
    "len(train_set), len(valid_set), len(test_set)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(302, 100, 100)"
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "def get_threshold(text, label):\r\n",
    "    sentences = sent_tokenize(text)\r\n",
    "    embeddings = model.encode(sentences)\r\n",
    "\r\n",
    "    cos_sim = util.cos_sim(query_embeddings, embeddings)\r\n",
    "\r\n",
    "    all_sentence_combinations = []\r\n",
    "    for i in range(len(queries)):\r\n",
    "        for j in range(len(sentences)):\r\n",
    "            all_sentence_combinations.append([cos_sim[i][j], i, j])\r\n",
    "\r\n",
    "    #Sort list by the highest cosine similarity score\r\n",
    "    all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\r\n",
    "\r\n",
    "    output_pos_max = [0, 0, 0, 0, 0]\r\n",
    "    output_neg_max = [0, 0, 0, 0, 0]\r\n",
    "    output_pos_min = [1, 1, 1, 1, 1]\r\n",
    "    output_neg_min = [1, 1, 1, 1, 1]\r\n",
    "    for score, i, j in all_sentence_combinations:\r\n",
    "        if label[i] == 1:\r\n",
    "            output_pos_max[i] = max(score.item(), output_pos_max[i])\r\n",
    "            output_pos_min[i] = min(score.item(), output_pos_min[i])\r\n",
    "        else:\r\n",
    "            output_neg_max[i] = max(score.item(), output_neg_max[i])\r\n",
    "            output_neg_min[i] = min(score.item(), output_neg_min[i])\r\n",
    "    \r\n",
    "    for i in range(0, 5):\r\n",
    "        if output_pos_min[i] == 1:\r\n",
    "            output_pos_min[i] = 0\r\n",
    "        if output_neg_min[i] == 1:\r\n",
    "            output_neg_min[i] = 0\r\n",
    "    \r\n",
    "    return output_pos_max, output_neg_max, output_pos_min, output_neg_min"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "import numpy as np\r\n",
    "threshold_sum_pos_max = []\r\n",
    "threshold_sum_neg_max = []\r\n",
    "threshold_sum_pos_min = []\r\n",
    "threshold_sum_neg_min = []\r\n",
    "pos_num = np.array([0, 0, 0, 0, 0])\r\n",
    "\r\n",
    "\r\n",
    "for i in range(0, len(train_set)):\r\n",
    "    pos_max, neg_max, pos_min, neg_min = get_threshold(train_set[i][0], train_set[i][1])\r\n",
    "    threshold_sum_pos_max.append(pos_max)\r\n",
    "    threshold_sum_neg_max.append(neg_max)\r\n",
    "    threshold_sum_pos_min.append(pos_min)\r\n",
    "    threshold_sum_neg_min.append(neg_min)\r\n",
    "    pos_num += np.array(train_set[i][1])\r\n",
    "\r\n",
    "neg_num = np.array([len(train_set)] *5) - pos_num\r\n",
    "\r\n",
    "threshold_sum_pos_max = np.array(threshold_sum_pos_max)\r\n",
    "\r\n",
    "threshold_sum_pos_max.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(302, 5)"
      ]
     },
     "metadata": {},
     "execution_count": 104
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "threshold = [(max(threshold_sum_pos_max[:,i]) - min(threshold_sum_pos_max[:,i]))*0.8 + min(threshold_sum_pos_max[:,i]) for i in range(0,5)]\r\n",
    "threshold"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.31803216934204104,\n",
       " 0.30239362716674806,\n",
       " 0.27441158294677737,\n",
       " 0.42875185012817385,\n",
       " 0.43044724464416506]"
      ]
     },
     "metadata": {},
     "execution_count": 107
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "\r\n",
    "preds = []\r\n",
    "truths = []\r\n",
    "for i in range(0, len(test_set)):\r\n",
    "    preds.append(find_frame(test_set[i][0], threshold))\r\n",
    "    truths.append(test_set[i][1])\r\n",
    "\r\n",
    "input_dict = {\"y_true\": truths, \"y_pred\": preds}\r\n",
    "evaluator.eval(input_dict)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Precision': [0.5, 0.125, 0.2, 1.0, 0],\n",
       " 'Recall': [0.06976744186046512,\n",
       "  0.03571428571428571,\n",
       "  0.09090909090909091,\n",
       "  0.015625,\n",
       "  0],\n",
       " 'F1': [0.12244897959183673,\n",
       "  0.05555555555555556,\n",
       "  0.12500000000000003,\n",
       "  0.03076923076923077,\n",
       "  0],\n",
       " 'Acc': [0.57, 0.66, 0.86, 0.37, 0.64]}"
      ]
     },
     "metadata": {},
     "execution_count": 108
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}