{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import ClimateDataset, TSVDataset\n",
    "\n",
    "dataset = \"./unlabelled_articles_17K/opinion_climate_all_with_bias.csv\"\n",
    "annotated_id_file = \"./annotated_data_500/pretty_0611_lcad.txt\"\n",
    "annotated_file = \"./annotated_data_500/0611_majority.json\"\n",
    "\n",
    "entire_dataset = ClimateDataset(dataset, annotated_id_file, annotated_file, True)\n",
    "labelled_dataset = TSVDataset('./annotated_data_500/final_dataset_v2.tsv', True, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Length Statitics (After Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(257, 86, 85)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "from utils import Evaluator\n",
    "\n",
    "fold = 5\n",
    "\n",
    "train_set = Subset(labelled_dataset, [i for i in range((-1 + fold)%5, len(labelled_dataset), 5)] + [i for i in range((0 + fold)%5, len(labelled_dataset), 5)] + [i for i in range((1 + fold)%5, len(labelled_dataset), 5)])\n",
    "valid_set = Subset(labelled_dataset, [i for i in range((2 + fold)%5, len(labelled_dataset), 5)])\n",
    "test_set  = Subset(labelled_dataset, [i for i in range((3 + fold)%5, len(labelled_dataset), 5)])\n",
    "\n",
    "evaluator = Evaluator(classifier='naive', mode='macro', detail=True)\n",
    "\n",
    "len(train_set), len(valid_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('record surge atmospheric concentrations earths atmosphere surged record meteorological organization wmolast increase average yearsresearchers combination human activities el nio weather phenomenon drove level yearsthis greenhouse gas bulletin produced wmo based measurements countries stations dotted globe measure concentrations warming gases including carbon dioxide methane nitrous oxidethe figures published wmo left atmosphere amounts absorbed earths sinks oceans biosphere average concentrations hit ppm largest increase network dr oksana tarasova chief wmos global atmosphere watch programme told bbc newsthe largest increase previous el nio ppm ppm average ten yearsel nio impacts amount carbon atmosphere causing droughts limit uptake plants treesemissions human sources slowed couple dr tarasova cumulative total atmosphere matters stays aloft active centuriesover report increase atmosphere times larger ice agerapidly increasing atmospheric levels gases potential study initiate unpredictable system leading severe ecological economic disruptionsthe study notes increase total radiative forcing warming greenhouse gasesgeological wise injection huge amount heat dr tarasovathe ten happen fast dont knowledge system bit worrisomeaccording experts time earth experienced comparable concentration ago mid pliocene era warmer sea levels melting greenland west antarctic ice sheetsother experts field atmospheric agreed wmo findings concernthe ppm growth rate extreme double growth rate decade prof euan nisbet royal holloway university london told bbc newsit urgent follow paris agreement switch rapidly fossil fuels signs happen air recording changeanother concern report continuing mysterious rise methane levels atmosphere larger average ten prof nisbet fear vicious cycle methane drives temperatures releases methane natural sourcesthe rapid increase methane expected paris agreement methane growth strongest tropics tropics carbon isotopes methane growth driven fossil fuels understand methane rising change feedback worryingthe implications atmospheric measurements targets agreed paris pact negative observersthe dont lie emitting reversed erik solheim head environmentwe solutions address challenge global political sense urgencythe report issued week ahead instalment talks bonn declaration president trump intends deal negotiators meeting germany aiming advance clarify rulebook paris agreementfollow matt twitter facebook',\n",
       " [1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tk = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def length_statistic(dataset):\n",
    "    length = 0\n",
    "    max_len = 0\n",
    "    min_len = 100000\n",
    "    if type(dataset) == ClimateDataset:\n",
    "        for i in range(0, len(dataset)):\n",
    "            temp = len(tk.tokenize(dataset[i]))\n",
    "            if max_len < temp:\n",
    "                max_len = temp\n",
    "            if min_len > temp:\n",
    "                min_len = temp\n",
    "            length += temp\n",
    "    else:\n",
    "        for i in range(0, len(dataset)):\n",
    "            temp = len(tk.tokenize(dataset[i][0]))\n",
    "            if max_len < temp:\n",
    "                max_len = temp\n",
    "            if min_len > temp:\n",
    "                min_len = temp\n",
    "            length += temp\n",
    "\n",
    "    average_length = length / len(dataset)\n",
    "\n",
    "    return {'min':min_len, 'max':max_len, 'average':average_length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min': 4, 'max': 3716, 'average': 877.1968556614819}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_statistic(entire_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min': 70, 'max': 626, 'average': 333.5239043824701}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_statistic(labelled_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'min': 141, 'max': 1221, 'average': 730.274834437086},\n",
       " {'min': 218, 'max': 1157, 'average': 697.52},\n",
       " {'min': 210, 'max': 1177, 'average': 664.2})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_statistic(train_set), length_statistic(valid_set), length_statistic(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your testimony is not serious. 5580\n",
      "'We should let our children be children' 5596\n",
      "Err501 7397\n",
      "Err501 7410\n",
      "Err501 7435\n",
      "Err501 7449\n",
      "Err501 7451\n",
      "Err501 7454\n",
      "Err501 7461\n",
      "Err501 7463\n",
      "Err501 7465\n",
      "The desperate pleas of asylum seekers who Remain in Mexico 10474\n",
      "How Democrats are prepping for their first debate 10479\n",
      "How Democrats are prepping for their first debate 10523\n",
      "Skip to main content  11094\n",
      "Err501 11891\n",
      "Err501 12133\n",
      "Err501 12187\n",
      "Click for more article by throngsman .. 15645\n",
      "The Latest Nearby states sending fire help to California 17082\n"
     ]
    }
   ],
   "source": [
    "# 6 empty news, why?\n",
    "for i in range(0, len(entire_dataset)):\n",
    "    if len(tk.tokenize(entire_dataset[i])) <= 10:\n",
    "        print(entire_dataset[i], i)\n",
    "\n",
    "# delete and manually fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_distribution(dataset):\n",
    "    labels = [0]*5\n",
    "\n",
    "    for i in range(0, len(dataset)):\n",
    "        for j in range(0, 5):\n",
    "            if dataset[i][1][j] == 1:\n",
    "                labels[j] += 1\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[255, 154, 44, 334, 202]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_distribution(labelled_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([125, 27, 159, 26, 78], [37, 8, 50, 7, 27], [30, 14, 59, 11, 22])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_distribution(train_set), label_distribution(valid_set), label_distribution(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_baseline(dataset):\n",
    "    pred = []\n",
    "    truth = []\n",
    "    for i in range(0, len(dataset)):\n",
    "        temp = []\n",
    "        for j in range(0,5):\n",
    "            if random.randint(0,1)==1:\n",
    "                temp.append(1)\n",
    "            else:\n",
    "                temp.append(0)\n",
    "        pred.append(temp)\n",
    "        truth.append(dataset[i][1])\n",
    "\n",
    "    return {\"y_true\": truth, \"y_pred\": pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Precision': [0.5025641025641026,\n",
       "  0.10407239819004525,\n",
       "  0.5707762557077626,\n",
       "  0.11274509803921569,\n",
       "  0.29850746268656714],\n",
       " 'Recall': [0.5104166666666666,\n",
       "  0.46938775510204084,\n",
       "  0.4664179104477612,\n",
       "  0.5227272727272727,\n",
       "  0.47244094488188976],\n",
       " 'F1': [0.5064599483204135,\n",
       "  0.17037037037037037,\n",
       "  0.513347022587269,\n",
       "  0.18548387096774194,\n",
       "  0.3658536585365853],\n",
       " 'Acc': [0.5537383177570093,\n",
       "  0.4766355140186916,\n",
       "  0.4462616822429907,\n",
       "  0.5280373831775701,\n",
       "  0.514018691588785]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dict = random_baseline(labelled_dataset)\n",
    "\n",
    "evaluator.eval(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Precision': [0.48091603053435117,\n",
       "  0.10714285714285714,\n",
       "  0.5887096774193549,\n",
       "  0.11764705882352941,\n",
       "  0.304],\n",
       " 'Recall': [0.504,\n",
       "  0.4444444444444444,\n",
       "  0.4591194968553459,\n",
       "  0.6153846153846154,\n",
       "  0.48717948717948717],\n",
       " 'F1': [0.4921875,\n",
       "  0.17266187050359713,\n",
       "  0.5159010600706714,\n",
       "  0.19753086419753085,\n",
       "  0.37438423645320196],\n",
       " 'Acc': [0.49416342412451364,\n",
       "  0.5525291828793775,\n",
       "  0.4669260700389105,\n",
       "  0.49416342412451364,\n",
       "  0.5058365758754864]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dict = random_baseline(train_set)\n",
    "\n",
    "evaluator.eval(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Precision': [0.3333333333333333,\n",
       "  0.08108108108108109,\n",
       "  0.6382978723404256,\n",
       "  0.1794871794871795,\n",
       "  0.375],\n",
       " 'Recall': [0.40540540540540543, 0.375, 0.6, 1.0, 0.6666666666666666],\n",
       " 'F1': [0.36585365853658536,\n",
       "  0.13333333333333333,\n",
       "  0.6185567010309279,\n",
       "  0.30434782608695654,\n",
       "  0.4800000000000001],\n",
       " 'Acc': [0.3953488372093023,\n",
       "  0.5465116279069767,\n",
       "  0.5697674418604651,\n",
       "  0.627906976744186,\n",
       "  0.5465116279069767]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dict = random_baseline(valid_set)\n",
    "\n",
    "evaluator.eval(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Precision': [0.42105263157894735,\n",
       "  0.09523809523809523,\n",
       "  0.75,\n",
       "  0.07142857142857142,\n",
       "  0.27906976744186046],\n",
       " 'Recall': [0.5333333333333333,\n",
       "  0.2857142857142857,\n",
       "  0.5084745762711864,\n",
       "  0.2727272727272727,\n",
       "  0.5454545454545454],\n",
       " 'F1': [0.47058823529411764,\n",
       "  0.14285714285714285,\n",
       "  0.6060606060606061,\n",
       "  0.11320754716981131,\n",
       "  0.36923076923076914],\n",
       " 'Acc': [0.5764705882352941,\n",
       "  0.43529411764705883,\n",
       "  0.5411764705882353,\n",
       "  0.4470588235294118,\n",
       "  0.5176470588235295]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dict = random_baseline(test_set)\n",
    "\n",
    "evaluator.eval(input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_voting(train, dataset):\n",
    "    labels = [0]*5\n",
    "\n",
    "    for i in range(0, len(train)):\n",
    "        for j in range(0, 5):\n",
    "            if train[i][1][j] == 1:\n",
    "                labels[j] += 1\n",
    "\n",
    "    trans = [1] * 5\n",
    "\n",
    "    for i in range(0, 5):\n",
    "        if labels[i] > (len(train_set)  // 2):\n",
    "            trans[i] = 1\n",
    "        else:\n",
    "            trans[i] = 0\n",
    "    pred = []\n",
    "    truth = []\n",
    "    for i in range(0, len(dataset)):\n",
    "        pred.append(trans)\n",
    "        truth.append(dataset[i][1])\n",
    "    \n",
    "    return {\"y_true\": truth, \"y_pred\": pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Precision': [0, 0, 0.5813953488372093, 0, 0],\n",
       " 'Recall': [0, 0, 1.0, 0, 0],\n",
       " 'F1': [0, 0, 0.7352941176470588, 0, 0],\n",
       " 'Acc': [0.5697674418604651,\n",
       "  0.9069767441860465,\n",
       "  0.5813953488372093,\n",
       "  0.9186046511627907,\n",
       "  0.686046511627907]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dict = majority_voting(train_set, valid_set)\n",
    "\n",
    "evaluator.eval(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Precision': [0, 0, 0.6941176470588235, 0, 0],\n",
       " 'Recall': [0, 0, 1.0, 0, 0],\n",
       " 'F1': [0, 0, 0.8194444444444444, 0, 0],\n",
       " 'Acc': [0.6470588235294118,\n",
       "  0.8352941176470589,\n",
       "  0.6941176470588235,\n",
       "  0.8705882352941177,\n",
       "  0.7411764705882353]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dict = majority_voting(train_set, test_set)\n",
    "\n",
    "evaluator.eval(input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "balancing = False\n",
    "label = 'ec'\n",
    "\n",
    "train_corpus = []\n",
    "valid_corpus = []\n",
    "test_corpus  = []\n",
    "\n",
    "train_labels = {'ar':[], 'co':[], 'ec':[], 'mo':[], 'hi':[]}\n",
    "valid_labels = {'ar':[], 'co':[], 'ec':[], 'mo':[], 'hi':[]}\n",
    "test_labels  = {'ar':[], 'co':[], 'ec':[], 'mo':[], 'hi':[]}\n",
    "\n",
    "for i in range(len(train_set)):\n",
    "    train_corpus.append(train_set[i][0])\n",
    "    train_labels['ar'].append(int(train_set[i][1][0]==1))\n",
    "    train_labels['hi'].append(int(train_set[i][1][1]==1))\n",
    "    train_labels['co'].append(int(train_set[i][1][2]==1))\n",
    "    train_labels['mo'].append(int(train_set[i][1][3]==1))\n",
    "    train_labels['ec'].append(int(train_set[i][1][4]==1))\n",
    "\n",
    "\n",
    "def dataset_balancing(specified_label):\n",
    "    positives = []\n",
    "    negatives = []\n",
    "    for i in range(0, len(train_labels[specified_label])):\n",
    "        if train_labels[specified_label][i] == 1:\n",
    "            positives.append(i)\n",
    "        else:\n",
    "            negatives.append(i)\n",
    "\n",
    "    if len(positives) > len(negatives):\n",
    "        for i in range(len(negatives), len(positives)):\n",
    "            choice = np.random.choice(negatives)\n",
    "            train_corpus.append(train_corpus[choice])\n",
    "            train_labels[specified_label].append(train_labels[specified_label][choice])\n",
    "    else:\n",
    "        for i in range(len(positives), len(negatives)):\n",
    "            choice = np.random.choice(positives)\n",
    "            train_corpus.append(train_corpus[choice])\n",
    "            train_labels[specified_label].append(train_labels[specified_label][choice])\n",
    "\n",
    "dataset_balancing(label)\n",
    "\n",
    "for i in range(len(valid_set)):\n",
    "    valid_corpus.append(valid_set[i][0])\n",
    "    valid_labels['ar'].append(int(valid_set[i][1][0]==1))\n",
    "    valid_labels['hi'].append(int(valid_set[i][1][1]==1))\n",
    "    valid_labels['co'].append(int(valid_set[i][1][2]==1))\n",
    "    valid_labels['mo'].append(int(valid_set[i][1][3]==1))\n",
    "    valid_labels['ec'].append(int(valid_set[i][1][4]==1))\n",
    "\n",
    "for i in range(len(test_set)):\n",
    "    test_corpus.append(test_set[i][0])\n",
    "    test_labels['ar'].append(int(test_set[i][1][0]==1))\n",
    "    test_labels['hi'].append(int(test_set[i][1][1]==1))\n",
    "    test_labels['co'].append(int(test_set[i][1][2]==1))\n",
    "    test_labels['mo'].append(int(test_set[i][1][3]==1))\n",
    "    test_labels['ec'].append(int(test_set[i][1][4]==1))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "train_vectors = vectorizer.fit_transform(train_corpus)\n",
    "valid_vectors = vectorizer.transform(valid_corpus)\n",
    "test_vectors = vectorizer.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label-ec-neigbours-2: precision:0.5769230769230769, recall:0.5555555555555556, f1:0.5660377358490566, acc:0.7325581395348837\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from dataset import LABELS\n",
    "\n",
    "\n",
    "best_metric = {'label': '', 'neighbors':'', 'precision': -1, 'recall':-1, 'f1':-1, 'acc':-1}\n",
    "for i in range(1, 20):\n",
    "\n",
    "    classifier = KNeighborsClassifier(n_neighbors=i)\n",
    "    classifier.fit(train_vectors, train_labels[label])\n",
    "\n",
    "    preds = classifier.predict(valid_vectors)\n",
    "\n",
    "    precision = precision_score(valid_labels[label], preds)\n",
    "    recall = recall_score(valid_labels[label], preds)\n",
    "    f1 = f1_score(valid_labels[label], preds)\n",
    "    acc = accuracy_score(valid_labels[label], preds)\n",
    "\n",
    "    if f1 > best_metric['f1']:\n",
    "        best_metric = {'label': label, 'neighbors': i, 'precision': precision, 'recall':recall, 'f1':f1, 'acc':acc}\n",
    "\n",
    "print('label-{}-neigbours-{}: precision:{}, recall:{}, f1:{}, acc:{}'.format(best_metric['label'], best_metric['neighbors'], best_metric['precision'], best_metric['recall'], best_metric['f1'], best_metric['acc']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold-5-label-ec: precision:0.3548387096774194, recall:0.5, f1:0.41509433962264153, acc:0.6352941176470588\n"
     ]
    }
   ],
   "source": [
    "classifier = KNeighborsClassifier(n_neighbors=best_metric['neighbors'])\n",
    "classifier.fit(train_vectors, train_labels[label])\n",
    "\n",
    "preds = classifier.predict(test_vectors)\n",
    "\n",
    "precision = precision_score(test_labels[label], preds)\n",
    "recall = recall_score(test_labels[label], preds)\n",
    "f1 = f1_score(test_labels[label], preds)\n",
    "acc = accuracy_score(test_labels[label], preds)\n",
    "\n",
    "print('fold-{}-label-{}: precision:{}, recall:{}, f1:{}, acc:{}'.format(fold, label, precision, recall, f1, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold-3-label-ar: precision:0.6538461538461539, recall:0.3695652173913043, f1:0.4722222222222222, acc:0.5581395348837209\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "label = 'ar'\n",
    "\n",
    "classifier = LogisticRegression(random_state=42)\n",
    "\n",
    "classifier.fit(train_vectors, train_labels[label])\n",
    "\n",
    "preds = classifier.predict(test_vectors)\n",
    "\n",
    "precision = precision_score(test_labels[label], preds)\n",
    "recall = recall_score(test_labels[label], preds)\n",
    "f1 = f1_score(test_labels[label], preds)\n",
    "acc = accuracy_score(test_labels[label], preds)\n",
    "\n",
    "print('fold-{}-label-{}: precision:{}, recall:{}, f1:{}, acc:{}'.format(fold, label, precision, recall, f1, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold-5-label-hi: precision:0.14285714285714285, recall:0.029411764705882353, f1:0.04878048780487805, acc:0.61\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "label = 'hi'\n",
    "\n",
    "classifier = GaussianNB()\n",
    "\n",
    "classifier.fit(train_vectors, train_labels[label])\n",
    "\n",
    "preds = classifier.predict(test_vectors)\n",
    "\n",
    "precision = precision_score(test_labels[label], preds)\n",
    "recall = recall_score(test_labels[label], preds)\n",
    "f1 = f1_score(test_labels[label], preds)\n",
    "acc = accuracy_score(test_labels[label], preds)\n",
    "\n",
    "print('fold-{}-label-{}: precision:{}, recall:{}, f1:{}, acc:{}'.format(fold, label, precision, recall, f1, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold-5-label-hi: precision:0.0, recall:0.0, f1:0.0, acc:0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "label = 'hi'\n",
    "\n",
    "classifier = RandomForestClassifier(max_depth=3, random_state=42)\n",
    "\n",
    "classifier.fit(train_vectors, train_labels[label])\n",
    "\n",
    "preds = classifier.predict(test_vectors)\n",
    "\n",
    "precision = precision_score(test_labels[label], preds)\n",
    "recall = recall_score(test_labels[label], preds)\n",
    "f1 = f1_score(test_labels[label], preds)\n",
    "acc = accuracy_score(test_labels[label], preds)\n",
    "\n",
    "print('fold-{}-label-{}: precision:{}, recall:{}, f1:{}, acc:{}'.format(fold, label, precision, recall, f1, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold-5-label-hi: precision:0.0, recall:0.0, f1:0.0, acc:0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "label = 'hi'\n",
    "\n",
    "classifier = SVC(gamma='auto')\n",
    "\n",
    "classifier.fit(train_vectors, train_labels[label])\n",
    "\n",
    "preds = classifier.predict(test_vectors)\n",
    "\n",
    "precision = precision_score(test_labels[label], preds)\n",
    "recall = recall_score(test_labels[label], preds)\n",
    "f1 = f1_score(test_labels[label], preds)\n",
    "acc = accuracy_score(test_labels[label], preds)\n",
    "\n",
    "print('fold-{}-label-{}: precision:{}, recall:{}, f1:{}, acc:{}'.format(fold, label, precision, recall, f1, acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
