{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Load Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from dataset import ClimateDataset\r\n",
    "\r\n",
    "dataset = \"./unlabelled_articles_17K/opinion_climate_all_with_bias.csv\"\r\n",
    "annotated_id_file = \"./annotated_data_500/pretty_0611_lcad.txt\"\r\n",
    "annotated_file = \"./annotated_data_500/0611_majority.json\"\r\n",
    "\r\n",
    "entire_dataset = ClimateDataset(dataset, annotated_id_file, annotated_file, True)\r\n",
    "labelled_dataset = entire_dataset.get_labelled_dataset()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Document Length Statitics (After Tokenization)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from torch.utils.data import Subset\r\n",
    "from utils import Evaluator\r\n",
    "\r\n",
    "fold = 5\r\n",
    "\r\n",
    "train_set = Subset(labelled_dataset, [i for i in range((-1 + fold)%5, len(labelled_dataset), 5)] + [i for i in range((0 + fold)%5, len(labelled_dataset), 5)] + [i for i in range((1 + fold)%5, len(labelled_dataset), 5)])\r\n",
    "valid_set = Subset(labelled_dataset, [i for i in range((2 + fold)%5, len(labelled_dataset), 5)])\r\n",
    "test_set  = Subset(labelled_dataset, [i for i in range((3 + fold)%5, len(labelled_dataset), 5)])\r\n",
    "\r\n",
    "evaluator = Evaluator(classifier='naive', mode='macro', detail=True)\r\n",
    "\r\n",
    "len(train_set), len(valid_set), len(test_set)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(302, 100, 100)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## length"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from transformers import BertTokenizer\r\n",
    "tk = BertTokenizer.from_pretrained(\"bert-base-uncased\")\r\n",
    "\r\n",
    "def length_statistic(dataset):\r\n",
    "    length = 0\r\n",
    "    max_len = 0\r\n",
    "    min_len = 100000\r\n",
    "    if type(dataset) == ClimateDataset:\r\n",
    "        for i in range(0, len(dataset)):\r\n",
    "            temp = len(tk.tokenize(dataset[i]))\r\n",
    "            if max_len < temp:\r\n",
    "                max_len = temp\r\n",
    "            if min_len > temp:\r\n",
    "                min_len = temp\r\n",
    "            length += temp\r\n",
    "    else:\r\n",
    "        for i in range(0, len(dataset)):\r\n",
    "            temp = len(tk.tokenize(dataset[i][0]))\r\n",
    "            if max_len < temp:\r\n",
    "                max_len = temp\r\n",
    "            if min_len > temp:\r\n",
    "                min_len = temp\r\n",
    "            length += temp\r\n",
    "\r\n",
    "    average_length = length / len(dataset)\r\n",
    "\r\n",
    "    return {'min':min_len, 'max':max_len, 'average':average_length}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "length_statistic(entire_dataset)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'min': 4, 'max': 3716, 'average': 877.1968556614819}"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "length_statistic(labelled_dataset)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'min': 70, 'max': 626, 'average': 333.5239043824701}"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "length_statistic(train_set), length_statistic(valid_set), length_statistic(test_set)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "({'min': 141, 'max': 1221, 'average': 730.274834437086},\n",
       " {'min': 218, 'max': 1157, 'average': 697.52},\n",
       " {'min': 210, 'max': 1177, 'average': 664.2})"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# 6 empty news, why?\r\n",
    "for i in range(0, len(entire_dataset)):\r\n",
    "    if len(tk.tokenize(entire_dataset[i])) <= 10:\r\n",
    "        print(entire_dataset[i], i)\r\n",
    "\r\n",
    "# delete and manually fixed"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Your testimony is not serious. 5580\n",
      "'We should let our children be children' 5596\n",
      "Err501 7397\n",
      "Err501 7410\n",
      "Err501 7435\n",
      "Err501 7449\n",
      "Err501 7451\n",
      "Err501 7454\n",
      "Err501 7461\n",
      "Err501 7463\n",
      "Err501 7465\n",
      "The desperate pleas of asylum seekers who Remain in Mexico 10474\n",
      "How Democrats are prepping for their first debate 10479\n",
      "How Democrats are prepping for their first debate 10523\n",
      "Skip to main content  11094\n",
      "Err501 11891\n",
      "Err501 12133\n",
      "Err501 12187\n",
      "Click for more article by throngsman .. 15645\n",
      "The Latest Nearby states sending fire help to California 17082\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## labels"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "def label_distribution(dataset):\r\n",
    "    labels = [0]*5\r\n",
    "\r\n",
    "    for i in range(0, len(dataset)):\r\n",
    "        for j in range(0, 5):\r\n",
    "            if dataset[i][1][j] == 1:\r\n",
    "                labels[j] += 1\r\n",
    "\r\n",
    "    return labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "label_distribution(labelled_dataset)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[255, 154, 44, 334, 202]"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "label_distribution(train_set), label_distribution(valid_set), label_distribution(test_set)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([148, 88, 25, 201, 113], [53, 32, 5, 62, 46], [54, 34, 14, 71, 43])"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Naive Baselines"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Baseline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "import random\r\n",
    "\r\n",
    "def random_baseline(dataset):\r\n",
    "    pred = []\r\n",
    "    truth = []\r\n",
    "    for i in range(0, len(dataset)):\r\n",
    "        temp = []\r\n",
    "        for j in range(0,5):\r\n",
    "            temp.append(random.randint(0,1))\r\n",
    "        pred.append(temp)\r\n",
    "        truth.append(dataset[i][1])\r\n",
    "\r\n",
    "    return {\"y_true\": truth, \"y_pred\": pred}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "input_dict = random_baseline(labelled_dataset)\r\n",
    "\r\n",
    "evaluator.eval(input_dict)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Precision': [0.5397489539748954,\n",
       "  0.3346456692913386,\n",
       "  0.0778688524590164,\n",
       "  0.6706827309236948,\n",
       "  0.366412213740458],\n",
       " 'Recall': [0.5058823529411764,\n",
       "  0.551948051948052,\n",
       "  0.4318181818181818,\n",
       "  0.5,\n",
       "  0.4752475247524752],\n",
       " 'F1': [0.5222672064777327,\n",
       "  0.41666666666666663,\n",
       "  0.13194444444444445,\n",
       "  0.5728987993138938,\n",
       "  0.41379310344827586],\n",
       " 'Acc': [0.5298804780876494,\n",
       "  0.5258964143426295,\n",
       "  0.50199203187251,\n",
       "  0.5039840637450199,\n",
       "  0.4581673306772908]}"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "input_dict = random_baseline(train_set)\r\n",
    "\r\n",
    "evaluator.eval(input_dict)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Precision': 0.4010473615509209,\n",
       " 'Recall': 0.47460626563847963,\n",
       " 'F1': 0.40805327993078855}"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "input_dict = random_baseline(valid_set)\r\n",
    "\r\n",
    "evaluator.eval(input_dict)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Precision': [0.49019607843137253,\n",
       "  0.26666666666666666,\n",
       "  0.04,\n",
       "  0.62,\n",
       "  0.4883720930232558],\n",
       " 'Recall': [0.4716981132075472, 0.5, 0.4, 0.5, 0.45652173913043476],\n",
       " 'F1': [0.4807692307692308,\n",
       "  0.3478260869565218,\n",
       "  0.07272727272727272,\n",
       "  0.5535714285714285,\n",
       "  0.47191011235955055],\n",
       " 'Acc': [0.46534653465346537,\n",
       "  0.40594059405940597,\n",
       "  0.49504950495049505,\n",
       "  0.504950495049505,\n",
       "  0.5346534653465347]}"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "input_dict = random_baseline(test_set)\r\n",
    "\r\n",
    "evaluator.eval(input_dict)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Precision': [0.6226415094339622,\n",
       "  0.38596491228070173,\n",
       "  0.10869565217391304,\n",
       "  0.7083333333333334,\n",
       "  0.5471698113207547],\n",
       " 'Recall': [0.6111111111111112,\n",
       "  0.6470588235294118,\n",
       "  0.35714285714285715,\n",
       "  0.4788732394366197,\n",
       "  0.6744186046511628],\n",
       " 'F1': [0.616822429906542,\n",
       "  0.48351648351648346,\n",
       "  0.16666666666666666,\n",
       "  0.5714285714285715,\n",
       "  0.6041666666666666],\n",
       " 'Acc': [0.59, 0.53, 0.5, 0.49, 0.62]}"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Majority Voting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "def majority_voting(train, dataset):\r\n",
    "    labels = [0]*5\r\n",
    "\r\n",
    "    for i in range(0, len(train)):\r\n",
    "        for j in range(0, 5):\r\n",
    "            if train[i][1][j] == 1:\r\n",
    "                labels[j] += 1\r\n",
    "\r\n",
    "    trans = [1] * 5\r\n",
    "    for i in range(0, 5):\r\n",
    "        if labels[i] > (len(train_set)  // 2):\r\n",
    "            trans[i] = 1\r\n",
    "        else:\r\n",
    "            trans[i] = 0\r\n",
    "    pred = []\r\n",
    "    truth = []\r\n",
    "    for i in range(0, len(dataset)):\r\n",
    "        pred.append(trans)\r\n",
    "        truth.append(dataset[i][1])\r\n",
    "    \r\n",
    "    return {\"y_true\": truth, \"y_pred\": pred}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "input_dict = majority_voting(train_set, valid_set)\r\n",
    "\r\n",
    "evaluator.eval(input_dict)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Precision': 0.21400000000000002, 'Recall': 0.4, 'F1': 0.27637728125533007}"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "input_dict = majority_voting(train_set, test_set)\r\n",
    "\r\n",
    "evaluator.eval(input_dict)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Precision': [0, 0, 0, 0.71, 0],\n",
       " 'Recall': [0, 0, 0, 1.0, 0],\n",
       " 'F1': [0, 0, 0, 0.8304093567251462, 0],\n",
       " 'Acc': [0.46, 0.66, 0.86, 0.71, 0.57]}"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TF-IDF"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "\r\n",
    "train_corpus = []\r\n",
    "valid_corpus = []\r\n",
    "test_corpus  = []\r\n",
    "\r\n",
    "train_labels = {'ar':[], 'co':[], 'ec':[], 'mo':[], 'hi':[]}\r\n",
    "valid_labels = {'ar':[], 'co':[], 'ec':[], 'mo':[], 'hi':[]}\r\n",
    "test_labels  = {'ar':[], 'co':[], 'ec':[], 'mo':[], 'hi':[]}\r\n",
    "\r\n",
    "for i in range(len(train_set)):\r\n",
    "    train_corpus.append(train_set[i][0])\r\n",
    "    train_labels['ar'].append(train_set[i][1][0])\r\n",
    "    train_labels['hi'].append(train_set[i][1][1])\r\n",
    "    train_labels['mo'].append(train_set[i][1][2])\r\n",
    "    train_labels['co'].append(train_set[i][1][3])\r\n",
    "    train_labels['ec'].append(train_set[i][1][4])\r\n",
    "\r\n",
    "for i in range(len(valid_set)):\r\n",
    "    valid_corpus.append(valid_set[i][0])\r\n",
    "    valid_labels['ar'].append(valid_set[i][1][0])\r\n",
    "    valid_labels['hi'].append(valid_set[i][1][1])\r\n",
    "    valid_labels['mo'].append(valid_set[i][1][2])\r\n",
    "    valid_labels['co'].append(valid_set[i][1][3])\r\n",
    "    valid_labels['ec'].append(valid_set[i][1][4])\r\n",
    "\r\n",
    "for i in range(len(test_set)):\r\n",
    "    test_corpus.append(test_set[i][0])\r\n",
    "    test_labels['ar'].append(test_set[i][1][0])\r\n",
    "    test_labels['hi'].append(test_set[i][1][1])\r\n",
    "    test_labels['mo'].append(test_set[i][1][2])\r\n",
    "    test_labels['co'].append(test_set[i][1][3])\r\n",
    "    test_labels['ec'].append(test_set[i][1][4])\r\n",
    "\r\n",
    "vectorizer = TfidfVectorizer()\r\n",
    "\r\n",
    "train_vectors = vectorizer.fit_transform(train_corpus)\r\n",
    "valid_vectors = vectorizer.transform(valid_corpus)\r\n",
    "test_vectors = vectorizer.transform(test_corpus)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### KNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\r\n",
    "from dataset import LABELS\r\n",
    "\r\n",
    "label = 'ar'\r\n",
    "\r\n",
    "best_metric = {'label': '', 'neighbors':'', 'precision': -1, 'recall':-1, 'f1':-1, 'acc':-1}\r\n",
    "for i in range(1, 20):\r\n",
    "\r\n",
    "    classifier = KNeighborsClassifier(n_neighbors=i)\r\n",
    "    classifier.fit(train_vectors, train_labels[label])\r\n",
    "\r\n",
    "    preds = classifier.predict(valid_vectors)\r\n",
    "\r\n",
    "    precision = precision_score(valid_labels[label], preds)\r\n",
    "    recall = recall_score(valid_labels[label], preds)\r\n",
    "    f1 = f1_score(valid_labels[label], preds)\r\n",
    "    acc = accuracy_score(valid_labels[label], preds)\r\n",
    "\r\n",
    "    if f1 > best_metric['f1']:\r\n",
    "        best_metric = {'label': label, 'neighbors': i, 'precision': precision, 'recall':recall, 'f1':f1, 'acc':acc}\r\n",
    "\r\n",
    "print('label-{}-neigbours-{}: precision:{}, recall:{}, f1:{}, acc:{}'.format(best_metric['label'], best_metric['neighbors'], best_metric['precision'], best_metric['recall'], best_metric['f1'], best_metric['acc']))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "label-ar-neigbours-19: precision:0.5853658536585366, recall:0.8888888888888888, f1:0.7058823529411764, acc:0.6\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "source": [
    "classifier = KNeighborsClassifier(n_neighbors=best_metric['neighbors'])\r\n",
    "classifier.fit(train_vectors, train_labels[label])\r\n",
    "\r\n",
    "preds = classifier.predict(test_vectors)\r\n",
    "\r\n",
    "precision = precision_score(test_labels[label], preds)\r\n",
    "recall = recall_score(test_labels[label], preds)\r\n",
    "f1 = f1_score(test_labels[label], preds)\r\n",
    "acc = accuracy_score(test_labels[label], preds)\r\n",
    "\r\n",
    "print('fold-{}-label-{}: precision:{}, recall:{}, f1:{}, acc:{}'.format(fold, label, precision, recall, f1, acc))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fold-5-label-ar: precision:0.45348837209302323, recall:0.9069767441860465, f1:0.6046511627906976, acc:0.49\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### LR"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "source": [
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\r\n",
    "\r\n",
    "label = 'ar'\r\n",
    "\r\n",
    "classifier = LogisticRegression(random_state=42)\r\n",
    "\r\n",
    "classifier.fit(train_vectors, train_labels[label])\r\n",
    "\r\n",
    "preds = classifier.predict(test_vectors)\r\n",
    "\r\n",
    "precision = precision_score(test_labels[label], preds)\r\n",
    "recall = recall_score(test_labels[label], preds)\r\n",
    "f1 = f1_score(test_labels[label], preds)\r\n",
    "acc = accuracy_score(test_labels[label], preds)\r\n",
    "\r\n",
    "print('fold-{}-label-{}: precision:{}, recall:{}, f1:{}, acc:{}'.format(fold, label, precision, recall, f1, acc))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fold-5-label-ar: precision:0.527027027027027, recall:0.7222222222222222, f1:0.609375, acc:0.5\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### NB"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "source": [
    "from sklearn.naive_bayes import GaussianNB\r\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\r\n",
    "\r\n",
    "label = 'hi'\r\n",
    "\r\n",
    "classifier = GaussianNB()\r\n",
    "\r\n",
    "classifier.fit(train_vectors, train_labels[label])\r\n",
    "\r\n",
    "preds = classifier.predict(test_vectors)\r\n",
    "\r\n",
    "precision = precision_score(test_labels[label], preds)\r\n",
    "recall = recall_score(test_labels[label], preds)\r\n",
    "f1 = f1_score(test_labels[label], preds)\r\n",
    "acc = accuracy_score(test_labels[label], preds)\r\n",
    "\r\n",
    "print('fold-{}-label-{}: precision:{}, recall:{}, f1:{}, acc:{}'.format(fold, label, precision, recall, f1, acc))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fold-5-label-hi: precision:0.14285714285714285, recall:0.029411764705882353, f1:0.04878048780487805, acc:0.61\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Random Forest"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\r\n",
    "\r\n",
    "label = 'hi'\r\n",
    "\r\n",
    "classifier = RandomForestClassifier(max_depth=3, random_state=42)\r\n",
    "\r\n",
    "classifier.fit(train_vectors, train_labels[label])\r\n",
    "\r\n",
    "preds = classifier.predict(test_vectors)\r\n",
    "\r\n",
    "precision = precision_score(test_labels[label], preds)\r\n",
    "recall = recall_score(test_labels[label], preds)\r\n",
    "f1 = f1_score(test_labels[label], preds)\r\n",
    "acc = accuracy_score(test_labels[label], preds)\r\n",
    "\r\n",
    "print('fold-{}-label-{}: precision:{}, recall:{}, f1:{}, acc:{}'.format(fold, label, precision, recall, f1, acc))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fold-5-label-hi: precision:0.0, recall:0.0, f1:0.0, acc:0.72\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### SVM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "source": [
    "from sklearn.svm import SVC\r\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\r\n",
    "\r\n",
    "label = 'hi'\r\n",
    "\r\n",
    "classifier = SVC(gamma='auto')\r\n",
    "\r\n",
    "classifier.fit(train_vectors, train_labels[label])\r\n",
    "\r\n",
    "preds = classifier.predict(test_vectors)\r\n",
    "\r\n",
    "precision = precision_score(test_labels[label], preds)\r\n",
    "recall = recall_score(test_labels[label], preds)\r\n",
    "f1 = f1_score(test_labels[label], preds)\r\n",
    "acc = accuracy_score(test_labels[label], preds)\r\n",
    "\r\n",
    "print('fold-{}-label-{}: precision:{}, recall:{}, f1:{}, acc:{}'.format(fold, label, precision, recall, f1, acc))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fold-5-label-hi: precision:0.0, recall:0.0, f1:0.0, acc:0.72\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}