{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils.dataset_processing import ClimateDataset, TSVDataset\n",
    "\n",
    "dataset = \"./data_utils/unlabelled_articles_17K/opinion_climate_all_with_bias.csv\"\n",
    "annotated_id_file = \"./data_utils/annotated_data_500/pretty_0611_lcad.txt\"\n",
    "annotated_file = \"./data_utils/annotated_data_500/0611_majority.json\"\n",
    "\n",
    "entire_dataset = ClimateDataset(dataset, annotated_id_file, annotated_file)\n",
    "labelled_dataset = TSVDataset('./data_utils/annotated_data_500/final_dataset_v2.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(257, 86, 85)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "from utils import Evaluator\n",
    "\n",
    "fold = 5\n",
    "\n",
    "train_set = Subset(labelled_dataset, [i for i in range((-1 + fold)%5, len(labelled_dataset), 5)] + [i for i in range((0 + fold)%5, len(labelled_dataset), 5)] + [i for i in range((1 + fold)%5, len(labelled_dataset), 5)])\n",
    "valid_set = Subset(labelled_dataset, [i for i in range((2 + fold)%5, len(labelled_dataset), 5)])\n",
    "test_set  = Subset(labelled_dataset, [i for i in range((3 + fold)%5, len(labelled_dataset), 5)])\n",
    "\n",
    "evaluator = Evaluator(classifier='naive', mode='macro', detail=True)\n",
    "\n",
    "len(train_set), len(valid_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: 109486464\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "total = sum([param.nelement() for param in model.parameters()])\n",
    "print('parameters:', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = ['attribution of responsibility', 'human interest', 'morality', 'conflict', 'economy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01343607  0.03816285  0.02155725 ...  0.03995542  0.0181604\n",
      "  -0.02009618]\n",
      " [ 0.01507804  0.08343946 -0.00394326 ...  0.05272075  0.00399472\n",
      "   0.01211869]\n",
      " [ 0.02066687  0.06011951  0.0280279  ...  0.0401177   0.03867239\n",
      "   0.0159173 ]\n",
      " [ 0.05329098  0.01754071 -0.00333661 ...  0.03607184  0.02508171\n",
      "   0.03295524]\n",
      " [-0.04011215  0.11893874  0.01274614 ... -0.01535319 -0.00672154\n",
      "   0.00540853]]\n"
     ]
    }
   ],
   "source": [
    "query_embeddings = model.encode(queries)\n",
    "\n",
    "print(query_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(labelled_dataset[2][0])\n",
    "embeddings = model.encode(sentences)\n",
    "print(labelled_dataset[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 most similar pairs:\n",
      "economy \t The situation is chaotic people are running on the streets and buildings collapsed. \t 0.2419\n",
      "conflict \t The situation is chaotic people are running on the streets and buildings collapsed. \t 0.2398\n",
      "conflict \t Video on social media shows people screaming and fleeing in panic and a mosque amongst the buildings damaged. \t 0.1565\n",
      "economy \t Last month a series of earthquakes struck the Indonesian island of Lombok killing hundreds of people the biggest on 5 August killed more than 460. \t 0.1465\n",
      "attribution of responsibility \t There is a ship washed ashore she added. \t 0.1386\n"
     ]
    }
   ],
   "source": [
    "cos_sim = util.cos_sim(query_embeddings, embeddings)\n",
    "\n",
    "all_sentence_combinations = []\n",
    "for i in range(len(queries)):\n",
    "    for j in range(len(sentences)):\n",
    "        all_sentence_combinations.append([cos_sim[i][j], i, j])\n",
    "\n",
    "all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print(\"Top-5 most similar pairs:\")\n",
    "for score, i, j in all_sentence_combinations[0:5]:\n",
    "    print(\"{} \\t {} \\t {:.4f}\".format(queries[i], sentences[j], score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero Shot Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "fold = 1\n",
    "test_set  = Subset(labelled_dataset, [i for i in range((3 + fold)%5, len(labelled_dataset), 5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_em(input_dict):\n",
    "    preds = input_dict['y_pred']\n",
    "    truths = input_dict['y_true']\n",
    "    correct_1 = 0\n",
    "    total_1 = 0\n",
    "    correct_2 = 0\n",
    "    total_2 = 0\n",
    "    correct_a = 0\n",
    "    total_a = 0\n",
    "\n",
    "    for i in range(len(truths)):\n",
    "        if sum(truths[i]) == 1:\n",
    "            total_1 += 1\n",
    "            if preds[i] == truths[i]:\n",
    "                correct_1 += 1\n",
    "\n",
    "        elif sum(truths[i]) == 2:\n",
    "            total_2 += 1\n",
    "            if preds[i] == truths[i]:\n",
    "                correct_2 += 1\n",
    "\n",
    "        total_a += 1\n",
    "        if preds[i] == truths[i]:\n",
    "            correct_a += 1\n",
    "\n",
    "    return {'em-1': correct_1/total_1, 'em-2': correct_2/total_2, 'em-a': correct_a/total_a}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_frame_with_best_n(text, n):\n",
    "    sentences = sent_tokenize(text)\n",
    "    embeddings = model.encode(sentences)\n",
    "\n",
    "    cos_sim = util.cos_sim(query_embeddings, embeddings)\n",
    "\n",
    "    all_sentence_combinations = []\n",
    "    for i in range(len(queries)):\n",
    "        for j in range(len(sentences)):\n",
    "            all_sentence_combinations.append([cos_sim[i][j], i, j])\n",
    "\n",
    "    #Sort list by the highest cosine similarity score\n",
    "    all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    output = [0, 0, 0, 0, 0]\n",
    "    for score, i, j in all_sentence_combinations[:n]:\n",
    "        output[i] = 1\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'em-1': 0.1, 'em-2': 0.07407407407407407, 'em-a': 0.04}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Precision': [0.34782608695652173,\n",
       "  0.20588235294117646,\n",
       "  0.09090909090909091,\n",
       "  0.6730769230769231,\n",
       "  0.4107142857142857],\n",
       " 'Recall': [0.18604651162790697,\n",
       "  0.25,\n",
       "  0.09090909090909091,\n",
       "  0.546875,\n",
       "  0.6571428571428571],\n",
       " 'F1': [0.24242424242424243,\n",
       "  0.22580645161290322,\n",
       "  0.09090909090909091,\n",
       "  0.603448275862069,\n",
       "  0.5054945054945055],\n",
       " 'Acc': [0.5, 0.52, 0.8, 0.54, 0.55]}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 2\n",
    "preds = []\n",
    "truths = []\n",
    "for i in range(0, len(test_set)):\n",
    "    preds.append(find_frame_with_best_n(test_set[i][0], n))\n",
    "    truths.append(test_set[i][1])\n",
    "\n",
    "input_dict = {\"y_true\": truths, \"y_pred\": preds}\n",
    "\n",
    "from utils import Evaluator\n",
    "\n",
    "evaluator = Evaluator(classifier='naive', mode='macro', detail=True)   \n",
    "print(calc_em(input_dict))\n",
    "evaluator.eval(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_frame(text, threshold):\n",
    "    sentences = sent_tokenize(text)\n",
    "    embeddings = model.encode(sentences)\n",
    "\n",
    "    cos_sim = util.cos_sim(query_embeddings, embeddings)\n",
    "\n",
    "    all_sentence_combinations = []\n",
    "    for i in range(len(queries)):\n",
    "        for j in range(len(sentences)):\n",
    "            all_sentence_combinations.append([cos_sim[i][j], i, j])\n",
    "\n",
    "    #Sort list by the highest cosine similarity score\n",
    "    all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    output = [0, 0, 0, 0, 0]\n",
    "    for score, i, j in all_sentence_combinations:\n",
    "        if score < threshold[i]:\n",
    "            break\n",
    "        else:\n",
    "            output[queries.index(queries[i])] = 1\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'em-1': 0.0, 'em-2': 0.030303030303030304, 'em-a': 0.0297029702970297}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Precision': [0.4810126582278481,\n",
       "  0.2619047619047619,\n",
       "  0.0847457627118644,\n",
       "  0.7444444444444445,\n",
       "  0.3723404255319149],\n",
       " 'Recall': [0.76, 0.7857142857142857, 1.0, 0.9305555555555556, 1.0],\n",
       " 'F1': [0.5891472868217054,\n",
       "  0.39285714285714285,\n",
       "  0.15625,\n",
       "  0.8271604938271605,\n",
       "  0.5426356589147286],\n",
       " 'Acc': [0.4752475247524752,\n",
       "  0.32673267326732675,\n",
       "  0.46534653465346537,\n",
       "  0.7227722772277227,\n",
       "  0.4158415841584158]}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = [0.17] * 5\n",
    "\n",
    "preds = []\n",
    "truths = []\n",
    "for i in range(0, len(test_set)):\n",
    "    preds.append(find_frame(test_set[i][0], threshold))\n",
    "    truths.append(test_set[i][1])\n",
    "\n",
    "input_dict = {\"y_true\": truths, \"y_pred\": preds}\n",
    "print(calc_em(input_dict))\n",
    "from utils import Evaluator\n",
    "\n",
    "evaluator = Evaluator(classifier='naive', mode='macro', detail=True)   \n",
    "\n",
    "evaluator.eval(input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weakly supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(302, 100, 100)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "from utils import Evaluator\n",
    "\n",
    "fold = 5\n",
    "\n",
    "train_set = Subset(labelled_dataset, [i for i in range((-1 + fold)%5, len(labelled_dataset), 5)] + [i for i in range((0 + fold)%5, len(labelled_dataset), 5)] + [i for i in range((1 + fold)%5, len(labelled_dataset), 5)])\n",
    "valid_set = Subset(labelled_dataset, [i for i in range((2 + fold)%5, len(labelled_dataset), 5)])\n",
    "test_set  = Subset(labelled_dataset, [i for i in range((3 + fold)%5, len(labelled_dataset), 5)])\n",
    "\n",
    "evaluator = Evaluator(classifier='naive', mode='macro', detail=True)\n",
    "\n",
    "len(train_set), len(valid_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_threshold(text, label):\n",
    "    sentences = sent_tokenize(text)\n",
    "    embeddings = model.encode(sentences)\n",
    "\n",
    "    cos_sim = util.cos_sim(query_embeddings, embeddings)\n",
    "\n",
    "    all_sentence_combinations = []\n",
    "    for i in range(len(queries)):\n",
    "        for j in range(len(sentences)):\n",
    "            all_sentence_combinations.append([cos_sim[i][j], i, j])\n",
    "\n",
    "    #Sort list by the highest cosine similarity score\n",
    "    all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    output_pos_max = [0, 0, 0, 0, 0]\n",
    "    output_neg_max = [0, 0, 0, 0, 0]\n",
    "    output_pos_min = [1, 1, 1, 1, 1]\n",
    "    output_neg_min = [1, 1, 1, 1, 1]\n",
    "    for score, i, j in all_sentence_combinations:\n",
    "        if label[i] == 1:\n",
    "            output_pos_max[i] = max(score.item(), output_pos_max[i])\n",
    "            output_pos_min[i] = min(score.item(), output_pos_min[i])\n",
    "        else:\n",
    "            output_neg_max[i] = max(score.item(), output_neg_max[i])\n",
    "            output_neg_min[i] = min(score.item(), output_neg_min[i])\n",
    "    \n",
    "    for i in range(0, 5):\n",
    "        if output_pos_min[i] == 1:\n",
    "            output_pos_min[i] = 0\n",
    "        if output_neg_min[i] == 1:\n",
    "            output_neg_min[i] = 0\n",
    "    \n",
    "    return output_pos_max, output_neg_max, output_pos_min, output_neg_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(302, 5)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "threshold_sum_pos_max = []\n",
    "threshold_sum_neg_max = []\n",
    "threshold_sum_pos_min = []\n",
    "threshold_sum_neg_min = []\n",
    "pos_num = np.array([0, 0, 0, 0, 0])\n",
    "\n",
    "\n",
    "for i in range(0, len(train_set)):\n",
    "    pos_max, neg_max, pos_min, neg_min = get_threshold(train_set[i][0], train_set[i][1])\n",
    "    threshold_sum_pos_max.append(pos_max)\n",
    "    threshold_sum_neg_max.append(neg_max)\n",
    "    threshold_sum_pos_min.append(pos_min)\n",
    "    threshold_sum_neg_min.append(neg_min)\n",
    "    pos_num += np.array(train_set[i][1])\n",
    "\n",
    "neg_num = np.array([len(train_set)] *5) - pos_num\n",
    "\n",
    "threshold_sum_pos_max = np.array(threshold_sum_pos_max)\n",
    "\n",
    "threshold_sum_pos_max.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.31803216934204104,\n",
       " 0.30239362716674806,\n",
       " 0.27441158294677737,\n",
       " 0.42875185012817385,\n",
       " 0.43044724464416506]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = [(max(threshold_sum_pos_max[:,i]) - min(threshold_sum_pos_max[:,i]))*0.8 + min(threshold_sum_pos_max[:,i]) for i in range(0,5)]\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Precision': [0.5, 0.125, 0.2, 1.0, 0],\n",
       " 'Recall': [0.06976744186046512,\n",
       "  0.03571428571428571,\n",
       "  0.09090909090909091,\n",
       "  0.015625,\n",
       "  0],\n",
       " 'F1': [0.12244897959183673,\n",
       "  0.05555555555555556,\n",
       "  0.12500000000000003,\n",
       "  0.03076923076923077,\n",
       "  0],\n",
       " 'Acc': [0.57, 0.66, 0.86, 0.37, 0.64]}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "preds = []\n",
    "truths = []\n",
    "for i in range(0, len(test_set)):\n",
    "    preds.append(find_frame(test_set[i][0], threshold))\n",
    "    truths.append(test_set[i][1])\n",
    "\n",
    "input_dict = {\"y_true\": truths, \"y_pred\": preds}\n",
    "evaluator.eval(input_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
